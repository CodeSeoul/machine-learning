{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification of digits using MNIST\n",
    "\n",
    "1. Data pre-processing\n",
    "2. Define Model\n",
    "3. Define loss function, optimizers, hyperparameters\n",
    "4. Define evaluation function\n",
    "5. Write up training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "After retrieving the dataset, we need to do the following steps\n",
    "\n",
    "1. Preprocess the data\n",
    "- Feature normalization\n",
    "- Convert PIL images into PyTorch tensors\n",
    "2. Divide the data into minibatches and shuffle the data using DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_mnist_dataloader(batch_size: int,\n",
    "                         is_training_dataset: bool) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Retrieve Mnist dataloader\n",
    "    Args:\n",
    "        batch_size: The batch size during training and evaluation\n",
    "        is_training_dataset: Set to true to retrieve training set. Otherwise, return\n",
    "    Returns:\n",
    "        A dataloader object\n",
    "    \"\"\"\n",
    "    # PIL Image (Python imaging library)\n",
    "    mnist_dataset = MNIST(root='.', download=True, train=is_training_dataset, transform=transforms.Compose([\n",
    "        # Convert PIL to tensor and normalize values between 0 and 1 by dividing all pixels by 255\n",
    "        # / 255\n",
    "        # Tanh function\n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "    \n",
    "    return DataLoader(mnist_dataset,\n",
    "                      # Size of mini-batch\n",
    "                      batch_size=batch_size,\n",
    "                      # Shuffle data each time during training to randomize samples\n",
    "                      shuffle=True,\n",
    "                      # We want to drop the last few to ensure that our batch_size remains constant.\n",
    "                      drop_last=True,\n",
    "                      # Pin memory speeds up the host to device (usually gpu in the real-world)\n",
    "                      # during training\n",
    "                      # Ideally, having all data on the GPU makes training faster, but most GPUs do not have enough\n",
    "                      # memory to host an entire dataset, so during training, we need to move the tensors from\n",
    "                      # cpu -> gpu to speed up operations\n",
    "                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "training_dataloader = get_mnist_dataloader(batch_size, is_training_dataset=True)\n",
    "test_dataloader = get_mnist_dataloader(batch_size, is_training_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model\n",
    "\n",
    "In this section, we will build a classification model. For this task, let's build a MLP (Multi-layer perceptron)\n",
    "made up of fully connected layers. To do so, we need to do the following.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        # define forward logic\n",
    "        pass\n",
    "```\n",
    "\n",
    "This is a great opportunity to fiddle around with PyTorch features. For starters, try building your models with `nn.Linear()`. For activation function, `LeakyReLU` might be a good start (with a slope value of 0.1 ~ 0.2). To make your model non-linear, try adding the activation function after each linear layer except for the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(\\nnn.Linear,\\nactivation,\\nnn.Linear,\\nactivation,\\n...\\nnn.Linear,\\n# No activation after final layer\\n)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your model code here\n",
    "# For starters, build your layer in the following format\n",
    "\"\"\"\n",
    "(\n",
    "nn.Linear,\n",
    "activation,\n",
    "nn.Linear,\n",
    "activation,\n",
    "...\n",
    "nn.Linear,\n",
    "# No activation after final layer\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Optimizers, Loss functions, etc.\n",
    "\n",
    "For the neural network to train, we need to choose the optimizer. The current silver bullet is `Adam`, but feel free to experiment with other optimizers. Since this is a multi-class classifier, we will be minimizing the cross entropy loss W.R.T the weights of the neural network.\n",
    "\n",
    "Setting the learning rate is kind of an art, but a good place to start would be `0.001` or `1e-3`. Try fiddling around with the learning rate to see what kind of result you get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function, optimizer, etc. \n",
    "# I recommend using Adam as a starting point. Feel free to experiment with learning rate and other \n",
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate accuracy of your model\n",
    "\n",
    "Traditionally, we would split the training set into a training and validation set and use the validation set to assess the progress of our training or the generalization capability of our model during training. \n",
    "\n",
    "The most commonly used technique is the [k-fold Cross Validation](https://machinelearningmastery.com/k-fold-cross-validation/) technique. \n",
    "\n",
    "The value `k` is a value representing the number of splits or sub-groups we will have. The main idea is to shuffle the dataset and split it into k groups. One of the k groups is used as validation data: the remaining is used to train the model.\n",
    "\n",
    "We repeat this process until each group in the k groups has been used as the validation dataset.\n",
    "\n",
    "Generally, `k = 5 or 10` is a good place to start. Remember with higher values of K, the lower the bias. It is also important to split the groups in such a way that we have a roughly balanced distribution of classes within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# No need to accumulate gradients during the evaluation phase\n",
    "# do this to reduce needless computation\n",
    "@torch.no_grad()\n",
    "def get_model_accuracy(model: nn.Module, dataloader: torch.utils.data.DataLoader) -> float:\n",
    "    # During evaluation, set the model's mode to eval to avoid\n",
    "    # unexpected surprises with layers such as Batch Normalization and Dropout\n",
    "    # since they behave differently during training and inference phase.\n",
    "    model.eval()\n",
    "    \n",
    "    for x_eval, y_eval in tqdm(dataloader):\n",
    "        # Define your evaluation logic right here\n",
    "        pass\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]\u001B[A\n",
      "  5%|▍         | 92/1875 [00:00<00:01, 912.30it/s]\u001B[A\n",
      " 11%|█         | 200/1875 [00:00<00:01, 1008.03it/s]\u001B[A\n",
      " 17%|█▋        | 310/1875 [00:00<00:01, 1049.50it/s]\u001B[A\n",
      " 23%|██▎       | 422/1875 [00:00<00:01, 1074.81it/s]\u001B[A\n",
      " 28%|██▊       | 534/1875 [00:00<00:01, 1088.38it/s]\u001B[A\n",
      " 34%|███▍      | 646/1875 [00:00<00:01, 1098.55it/s]\u001B[A\n",
      " 40%|████      | 758/1875 [00:00<00:01, 1103.48it/s]\u001B[A\n",
      " 46%|████▋     | 869/1875 [00:00<00:00, 1104.09it/s]\u001B[A\n",
      " 52%|█████▏    | 982/1875 [00:00<00:00, 1108.81it/s]\u001B[A\n",
      " 58%|█████▊    | 1093/1875 [00:01<00:00, 1108.01it/s]\u001B[A\n",
      " 64%|██████▍   | 1205/1875 [00:01<00:00, 1109.17it/s]\u001B[A\n",
      " 70%|███████   | 1316/1875 [00:01<00:00, 1107.10it/s]\u001B[A\n",
      " 76%|███████▌  | 1427/1875 [00:01<00:00, 1107.34it/s]\u001B[A\n",
      " 82%|████████▏ | 1538/1875 [00:01<00:00, 1105.71it/s]\u001B[A\n",
      " 88%|████████▊ | 1650/1875 [00:01<00:00, 1108.47it/s]\u001B[A\n",
      " 94%|█████████▍| 1762/1875 [00:01<00:00, 1110.81it/s]\u001B[A\n",
      "100%|██████████| 1875/1875 [00:01<00:00, 1094.09it/s]\u001B[A\n",
      "  0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 31\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# TODO: write your code here\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# -----------------------------------\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# TODO: Zero_grad your optimizer so you are not accumulating gradients during training phase\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m \n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# TODO: calculate Training accuracy\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m training_accuracy \u001B[38;5;241m=\u001B[39m get_model_accuracy(\u001B[43mmodel\u001B[49m, training_dataloader)\n\u001B[1;32m     32\u001B[0m training_accuracy \u001B[38;5;241m=\u001B[39m training_accuracy \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Convert floating point decimal to percentage.\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# e.g. 0.895 -> 89.5\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "area = width * height\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for step, (x_train, y_train) in enumerate(tqdm(training_dataloader)):\n",
    "        pass\n",
    "        # TODO: write your code here\n",
    "        # -----------------------------------\n",
    "        # TODO: Zero_grad your optimizer so you are not accumulating gradients during training phase\n",
    "    \n",
    "        # TODO: First, try flattening your images since\n",
    "        # our input dimensions of x_train are (batch_size, height, width)\n",
    "        # .view(), .reshape() or .flatten(start_dim=1) should work\n",
    "        # prefer reshape and flatten since it work on both contiguous and non-contiguous data\n",
    "        \n",
    "        # TODO: Afterwards, feed x_train to your model and obtain predictions\n",
    "        \n",
    "        # TODO: Good job, now calculate your loss using nn.CrossEntropyLoss \n",
    "        # For more information, look up the PyTorch documentation. You got this!\n",
    "        \n",
    "        # Once you have calculated your loss, do backprop on that loss. \n",
    "        # Afterwards, update the parameters of your model\n",
    "                \n",
    "        # TODO (Optional): For each n steps, log your loss and training progress\n",
    "        \n",
    "    \n",
    "    # TODO: calculate Training accuracy\n",
    "    training_accuracy = get_model_accuracy(model, training_dataloader)\n",
    "    training_accuracy = training_accuracy * 100\n",
    "    # Convert floating point decimal to percentage.\n",
    "    # e.g. 0.895 -> 89.5\n",
    "    print(f'Epoch {epoch + 1} model accuracy on training dataset: {training_accuracy:.2f}%')\n",
    "\n",
    "    # TODO: Calculate your evaluation metric on the evaluation dataset. We will use accuracy, which can be defined as\n",
    "    # follows: total number of correct predictions / total number of samples\n",
    "    # For now, we will use the test dataset as the evaluation dataset, but in the real-world\n",
    "    # make sure to use the k-fold cross validation\n",
    "    # Note: For each of these operations, in the real-world, try to keep each unit of operation in a single function\n",
    "    # to enable testing\n",
    "    test_accuracy = get_model_accuracy(model, test_dataloader)\n",
    "    test_accuracy = test_accuracy * 100\n",
    "    # Print out accuracy on test dataset\n",
    "    print(f'Epoch {epoch + 1} model accuracy on test dataset: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}