\documentclass{beamer}

\mode<presentation> {
\usetheme{Boadilla}
\usecolortheme{default}
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[USenglish,british,american,australian,english]{babel}
\begin{filecontents}{\jobname.bib}
@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@software{Novik_torchoptimizers,
    title        = {{torch-optimizer -- collection of optimization algorithms for PyTorch.}},
    author       = {Novik, Mykola},
    year         = 2020,
    month        = 1,
    version      = {1.0.1}
}
\end{filecontents}
\usepackage[style=numeric,backend=biber,autocite=plain,sorting=none]{biblatex}
\addbibresource{\jobname.bib}
  
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, 
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
%\usepackage{etoolbox} % for \ifthen
\usepackage{listofitems} % for \readlist to create arrays
\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

\tikzset{>=latex} % for LaTeX arrow head
\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]
\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]
\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]
\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]
\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round
\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1]
\tikzset{ % node styles, numbered for easy mapping with \nstyle
  node 1/.style={node in},
  node 2/.style={node hidden},
  node 3/.style={node out},
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\setbeamertemplate{caption}[numbered]
\AtBeginBibliography{\small}

%Includes "References" in the table of contents

\title[CodeSeoul] % (optional, only for long titles)
  {Optimization algorithms in deep learning}

\author[AI Research Paper Review] % (optional, for multiple authors)
  {Sanzhar Askaruly (San)}

\institute[] % (optional)
  { Ulsan National Institute of Science and Technology\newline
    Ph.D. Candidate in Biomedical Engineering}

\date[November 5]
{CodeSeoul ML Meetup \\November 5, 2022}

% some change
\begin{document}
    %\maketitle
    \begin{frame}
    \titlepage % Print the title page as the first slide
    \end{frame}

    % \begin{frame}
    % \frametitle{Overview} % Table of contents slide, comment this block out to remove it
    % \tableofcontents 
    % \end{frame}

    \begin{frame}{Overview}
      What we'll cover today:
      \tableofcontents
    \end{frame}
    
    \section{What is optimization?} %
    \begin{frame}{Optimization}
      \begin{center}
        \begin{huge}
          In \textit{context} of deep learning, \\
          goal is to \textbf{minimize loss function}
          \vspace{0.2cm}
          \begin{equation}
            \mathit{w}^* = \argmin_{w}L(w)
          \end{equation}
        \end{huge}
      \end{center}
    \end{frame}

    \begin{frame}{What is gradient descent optimization?}
      \begin{center}
          \includegraphics[width=0.8\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/what_is_gd.png}
      \end{center}
    \end{frame}


    \section{Algorithms} %

    \subsection{SGD (Stochastic gradient descent)}
    \begin{frame}
      \frametitle{Stochastic Gradient Descent (SGD)} % Table of contents slide, comment this block out to remove it
      \begin{block}{Algorithm}
        Update step ~\cite{ruder2016overview}:
        \begin{equation}
          \theta_{t+1} = \theta_{t} - \eta \cdot \nabla_{\theta}J(\theta_t)
        \end{equation}
        \vskip 0.3cm
        where, \\
        \begin{tabular}{l l}
          $\theta_{t}$: & current model parameters \\
          $\nabla_{\theta}J(\theta_t)$: & gradient of these model parameters \\
          $\eta$: & learning rate (fixed)
        \end{tabular}
        
      \end{block}
    \end{frame}



    \begin{frame}[fragile]
      \frametitle{Stochastic Gradient Descent (SGD)}
      \vspace{0.2cm}
      How we usually call in \verb|PyTorch|:
      \rule{\textwidth}{1pt}
      \scriptsize
      \begin{minted}{python}
        optimizer = optim.SGD(model.parameters(), lr=0.01)
      \end{minted}
      \rule{\textwidth}{1pt}
      
      \normalsize
      \vspace{0.2cm}
      How we can create our "native" class ~\cite{Novik_torchoptimizers}:
      \rule{\textwidth}{1pt}
      \scriptsize
      \begin{minted}{python}
        from torch.optim.optimizer import Optimizer
        
        class CustomSGD(Optimizer):
          def __init__(self, model_params, lr=1e-3):
              self.model_params = list(model_params)
              self.lr = lr

          def zero_grad(self):
              for param in self.model_params:
                  param.grad = None

          @torch.no_grad()
          def step(self):
              for param in self.model_params:
                  param.sub_(self.lr * param.grad)
      \end{minted}
      \rule{\textwidth}{1pt}
    \end{frame}


    \subsection{SGD with Momentum}
    \begin{frame}
      \frametitle{SGD with Momentum} % Table of contents slide, comment this block out to remove it
      
      \begin{columns}
          \begin{column}{0.5\textwidth}
            General idea:
            \begin{itemize}
              \item Overcome small gradients \\ near flat areas
              \item Build up from previous "velocity"
              \item Faster learning
            \end{itemize}
          \end{column}
          \begin{column}{0.5\textwidth}  %%<--- here
              \begin{center}
                \includegraphics[width=0.8\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/ball_momentum.jpeg}
              \end{center}
          \end{column}
      \end{columns}
    \end{frame}



    \begin{frame}
      \frametitle{SGD with Momentum}
      \begin{block}{Algorithm}

        Update step ~\cite{qian1999momentum}:
        \begin{equation}    % <--- deleted empty lines
          v_{t,i} = \gamma \cdot v_{t-1,i} + \nabla_{\theta}J(\theta_{t,i})
        \end{equation}
        \begin{equation}    % <--- deleted empty lines
          \theta_{t+1} = \theta_{t} - \eta \cdot v_{t,i}
        \end{equation}

        \vskip 0.3cm
        where, \\
        \begin{tabular}{l l}
          $\gamma$: & friction (or momentum, fixed) \\
          $v_{t}$: & velocity \\
          $\nabla_{\theta}J(\theta_t)$: & gradient of these model parameters \\
          $\eta$: & learning rate (fixed)
        \end{tabular}
      \end{block}
    \end{frame}



    \begin{frame}[fragile]
      \frametitle{SGD with Momentum}
      \vspace{0.2cm}
      How we can create "native" \verb|SGDMomentum| class:
      \rule{\textwidth}{1pt}
      \scriptsize
      \begin{minted}{python}
        from torch.optim.optimizer import Optimizer

        class CustomSGDMomentum(Optimizer):
          def __init__(self, model_params, lr=1e-3, momentum=0.9):
              self.model_params = list(model_params)
              self.lr = lr
              self.momentum = momentum
              self.v = [torch.zeros_like(p) for p in self.model_params]

          def zero_grad(self):
              for param in self.model_params:
                  param.grad = None

          @torch.no_grad()
          def step(self):
              for param, v in zip(self.model_params, self.v):
                  v.mul_(self.momentum).add_(param.grad)
                  param.sub_(self.lr * v)
      \end{minted}
      \rule{\textwidth}{1pt}
    \end{frame}



    \begin{frame}
      \begin{center}
      \frametitle{SGD with Momentum}
      \includegraphics[width=0.8\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/momentum.jpeg}
      \end{center}
    \end{frame}


    \subsection{Adagrad (Adaptive learning rate)}
    \begin{frame}
      \frametitle{Adagrad}
      \begin{block}{Algorithm}

        Update step ~\cite{duchi2011adaptive}:
        \begin{equation}    % <--- deleted empty lines
          \theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,i}}+ \epsilon} \cdot \nabla_{\theta}J(\theta_{t,i})
        \end{equation}
        where, \\
        \begin{equation}    % <--- deleted empty lines
          G_{t,i} = G_{t-1,i} + (\nabla_{\theta}J(\theta_{t,i}))^2
        \end{equation}
        

        \vskip 0.3cm
        and, \\
        \begin{tabular}{l l}
          $G_{t,i}$: & the sum of the squared gradients \\
          $\epsilon$: & a small number, to avoid division by zero\\
          $\theta_{t}$: & current model parameters \\
          $\nabla_{\theta}J(\theta_t)$: & gradient of these model parameters \\
          $\eta$: & learning rate (fixed)
        \end{tabular}
      \end{block}
    \end{frame}



    \begin{frame}[fragile]
      \frametitle{Adagrad}
      \vspace{0.2cm}
      How we can create "native" \verb|Adagrad| class:
      \rule{\textwidth}{1pt}
      \scriptsize
      \begin{minted}{python}
        from torch.optim.optimizer import Optimizer

        class CustomAdagrad(Optimizer):
          def __init__(self, model_params, lr=1e-2, init_acc_sqr_grad=0, eps=1e-10):
              self.model_params = list(model_params)
              self.lr = lr
              self.acc_sqr_grads = [torch.full_like(p, init_acc_sqr_grad) for p in self.model_params]
              self.eps = eps

          def zero_grad(self):
              for param in self.model_params:
                  param.grad = None

          @torch.no_grad()
          def step(self):
              for param, acc_sqr_grad in zip(self.model_params, self.acc_sqr_grads):
                  acc_sqr_grad.add_(param.grad * param.grad)
                  std = acc_sqr_grad.sqrt().add(self.eps)
                  param.sub_((self.lr / std) * param.grad)
      \end{minted}
      \rule{\textwidth}{1pt}
    \end{frame}

    
    \section{Experiments} %
    \begin{frame}{Experiment}
      \begin{flushleft}
        A vanilla MLP (Multilayer Perceptron)
      \end{flushleft}
    
      \begin{center}  
      % NEURAL NETWORK no text
      \begin{tikzpicture}[x=2.2cm,y=1.4cm]
        \message{^^JNeural network without text}
        \readlist\Nnod{4,5,5,5,3} % array of number of nodes per layer
        
        \message{^^J  Layer}
        \foreachitem \N \in \Nnod{ % loop over layers
          \def\lay{\Ncnt} % alias of index of current layer
          \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
          \message{\lay,}
          \foreach \i [evaluate={\y=\N/2-\i; \x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
            
            % NODES
            \node[node \n] (N\lay-\i) at (\x,\y) {};
            
            % CONNECTIONS
            \ifnum\lay>1 % connect to previous layer
              \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                \draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
                \draw[connect] (N\prev-\j) -- (N\lay-\i);
                %\draw[connect] (N\prev-\j.0) -- (N\lay-\i.180); % connect to left
              }
            \fi % else: nothing to connect first layer
            
          }
        }
        
        % LABELS
        \node[above=5,align=center,mygreen!60!black] at (N1-1.90) {input\\[-0.2em]layer};
        \node[above=2,align=center,myblue!60!black] at (N3-1.90) {hidden layer};
        \node[above=10,align=center,myred!60!black] at (N\Nnodlen-1.90) {output\\[-0.2em]layer};
        
      \end{tikzpicture}

      \end{center}
    \end{frame}
    

    \nocite{*}


    \begin{frame}{Experiment}
      \begin{flushleft}
        MNIST dataset ~\cite{lecun1998gradient}
      \end{flushleft}
      \vspace{0.1cm}
      \begin{center}
          \includegraphics[scale=0.4]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/mnist.png}
      \end{center}
    \end{frame}

    \begin{frame}{Experiment}
      \begin{figure}[htb]
        \begin{center}
          \includegraphics[width=1.0\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/sgd_mom.png}
        \end{center}
        \begin{center}
          \caption{SGD vs momentum [git: \href{https://github.com/CodeSeoul/machine-learning/tree/master/221105-optimization}{CodeSeoul/machine-learning}]}
        \end{center}
          %https://github.com/tuttelikz/221105-meetup-codeseoul
      \end{figure}
    \end{frame}

    \begin{frame}{Experiment}
      \begin{figure}[htb]
      \centering
        \includegraphics[width=1.0\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/curves.png}
        \caption{SGD vs momentum vs Adagrad [git: \href{https://github.com/CodeSeoul/machine-learning/tree/master/221105-optimization}{CodeSeoul/machine-learning}]}
        %https://github.com/tuttelikz/221105-meetup-codeseoul
      \end{figure}
    \end{frame}
    

    \section{Summary} %
    \subsection{Discussion}
    \begin{frame}{Discussion}  
      \begin{columns}
        \begin{column}{0.5\textwidth}  %%<--- here
          \begin{figure}[htb]
            \begin{center}
              \includegraphics[width=0.6\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/trajectory.png}
              \caption{Trajectories of optimization algorithms in high-dimensional space~\cite{ruder2016overview}}
            \end{center}
          \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
          \begin{figure}[htb]
            \begin{center}
              \includegraphics[width=0.7\textwidth]{/home/suzy/gitrepos/tuttelikz/221105/221105-optimization/images/optimal_learning_rate.jpeg}
              \caption{Selecting learning rate (lr)~\cite{mohanty_2019}}
            \end{center}
          \end{figure}
        \end{column}
    \end{columns}
      \begin{itemize}
        \item The randomness introduced by SGD allows to reach better minimum. But in cases with many local minima, SGD may still get stuck.
        \item A systematic way to choose a good lr is by initially assigning it a very low value and increasing it slowly until the loss stops decreasing.
      \end{itemize}

    \end{frame}


    \subsection{Practicum}
    \begin{frame}{Practicum}
      \begin{center}
      \begin{huge}Thank you for your attention!\end{huge}
      \end{center}

        \vspace{0.5cm}
        \begin{itemize}
          \item Workshop contents: \\
            \begin{small}\url{https://github.com/CodeSeoul/machine-learning/tree/master/221105-optimization}
            \end{small}
          \item Follow-up QA? \\ 
            \begin{small}\url{http://discord.com/users/tuttelikz}
            \end{small}
        \end{itemize}

    \end{frame}

    \begin{frame}{References}
      \printbibliography  
    \end{frame}

\end{document}
